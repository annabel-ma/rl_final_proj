{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0840fa96",
      "metadata": {
        "id": "0840fa96"
      },
      "source": [
        "\n",
        "# Statistical Testing for Deep RL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "tHCGklyqgxkS",
      "metadata": {
        "id": "tHCGklyqgxkS"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "import time\n",
        "import json\n",
        "from typing import Any, Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from scipy.stats import ttest_ind, mannwhitneyu, rankdata\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "Am_3FtwXgNeo",
      "metadata": {
        "id": "Am_3FtwXgNeo"
      },
      "outputs": [],
      "source": [
        "BASE = os.getcwd() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "458814e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistical testing configuration\n",
        "SEED_GRID = [2, 3, 5, 10, 20, 30] #[5, 10, 20, 30, 50, 100, 150, 200]  # sample sizes per group for FPR/Power analysis \n",
        "ALPHAS = [0.05, 0.01]  # Significance levels\n",
        "EPSILONS = [0.5, 1.0, 2.0]  # cohen's d effect sizes for power analysis\n",
        "N_RESAMPLES = 1000  # number of bootstrap/permutation resamples (reduce for faster iteration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ifBDMjrWg0Ex",
      "metadata": {
        "id": "ifBDMjrWg0Ex"
      },
      "outputs": [],
      "source": [
        "TASKS = [\n",
        "    \"Hopper-v5\",\n",
        "    \"Walker2d-v5\",\n",
        "    \"HalfCheetah-v5\",\n",
        "    \"Ant-v5\",\n",
        "    \"Humanoid-v5\",\n",
        "]\n",
        "\n",
        "ALGORITHMS = [\"SAC\", \"TD3\", \"DDPG\", \"PPO\"]\n",
        "\n",
        "EVAL_EPISODES = 20\n",
        "\n",
        "TIMESTEPS_PER_TASK = {\n",
        "    \"Hopper-v5\":      1_000_000,\n",
        "    \"Walker2d-v5\":    1_000_000,\n",
        "    \"HalfCheetah-v5\": 3_000_000,\n",
        "    \"Ant-v5\":         3_000_000,\n",
        "    \"Humanoid-v5\":   10_000_000,\n",
        "}\n",
        "\n",
        "DEFAULT_TOTAL_TIMESTEPS = 5_000_000\n",
        "\n",
        "BASE_DIR = os.path.join(BASE, \"rl_experiments\")\n",
        "RUNS_DIR = os.path.join(BASE_DIR, \"runs\")\n",
        "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
        "RESULTS_CSV = os.path.join(BASE_DIR, \"final_eval_returns.csv\")\n",
        "LEARNING_CURVES_CSV = os.path.join(BASE_DIR, \"learning_curves.csv\")\n",
        "\n",
        "os.makedirs(RUNS_DIR, exist_ok=True)\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "\n",
        "GLOBAL_RNG_SEED = 31415\n",
        "np.random.seed(GLOBAL_RNG_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "HWh1A-3K0tBn",
      "metadata": {
        "id": "HWh1A-3K0tBn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading from single CSV: /n/home09/annabelma/rl_final_proj/rl_experiments/learning_curves.csv\n",
            "  Loaded 46119 entries\n",
            "Loading from 3681 individual files in /n/home09/annabelma/rl_final_proj/rl_experiments/learning_curves\n",
            "  Loaded 368364 entries from individual files\n",
            "Total: 400384 entries after cleaning\n",
            "\n",
            "Loaded learning curves shape: (400384, 8)\n",
            "Unique tasks: ['Ant-v5', 'HalfCheetah-v5', 'Hopper-v5', 'Humanoid-v5', 'Walker2d-v5']\n",
            "Unique algorithms: ['DDPG', 'PPO', 'SAC', 'TD3']\n",
            "Total (task, algo, seed) combinations: 4020\n",
            "\n",
            "============================================================\n",
            "Checking for duplicate (task, algorithm, seed, env_steps) entries in curves...\n",
            "No duplicate (task, algorithm, seed, env_steps) entries found in curves\n",
            "\n",
            "============================================================\n",
            "Loading final evaluation returns...\n",
            "Loaded 5475 entries\n",
            "\n",
            "Found 2341 rows with duplicate (task, algorithm, seed) triples\n",
            "\n",
            "*** WARNING: 18 (task, algo, seed) groups have DIFFERENT final_return_mean values: ***\n",
            "                task algorithm  seed  final_return_mean     timestamp\n",
            "803           Ant-v5      DDPG     8        4005.700276  1.764646e+09\n",
            "864           Ant-v5      DDPG     8          -6.718302  1.764656e+09\n",
            "1046          Ant-v5      DDPG     8          -6.718302  1.764656e+09\n",
            "799   HalfCheetah-v5      DDPG     8        9374.516250  1.764646e+09\n",
            "862   HalfCheetah-v5      DDPG     8        6908.246865  1.764656e+09\n",
            "729        Hopper-v5      DDPG     8         849.586677  1.764639e+09\n",
            "742        Hopper-v5      DDPG     8        1213.358251  1.764640e+09\n",
            "1045       Hopper-v5      DDPG     8        1213.358251  1.764640e+09\n",
            "3743       Hopper-v5       SAC   182        2307.186004  1.765036e+09\n",
            "3803       Hopper-v5       SAC   182        3546.773665  1.765038e+09\n",
            "5370     Humanoid-v5      DDPG   146         129.072054  1.765294e+09\n",
            "5405     Humanoid-v5      DDPG   146          64.313453  1.765310e+09\n",
            "5364     Humanoid-v5      DDPG   147          63.284448  1.765293e+09\n",
            "5403     Humanoid-v5      DDPG   147          63.933435  1.765309e+09\n",
            "5435     Humanoid-v5      DDPG   199         213.390566  1.765336e+09\n",
            "5473     Humanoid-v5      DDPG   199          66.582618  1.765377e+09\n",
            "807      Humanoid-v5       PPO     8         770.992400  1.764647e+09\n",
            "882      Humanoid-v5       PPO     8         438.871257  1.764660e+09\n",
            "1050     Humanoid-v5       PPO     8         438.871257  1.764660e+09\n",
            "5465     Humanoid-v5       SAC   140        7030.837801  1.765359e+09\n",
            "5471     Humanoid-v5       SAC   140        7091.564919  1.765374e+09\n",
            "5416     Humanoid-v5       SAC   146        7998.487909  1.765323e+09\n",
            "5462     Humanoid-v5       SAC   146        6130.598259  1.765357e+09\n",
            "937      Humanoid-v5       TD3    27         252.823464  1.764679e+09\n",
            "944      Humanoid-v5       TD3    27         252.823464  1.764679e+09\n",
            "1098     Humanoid-v5       TD3    27         104.249579  1.764709e+09\n",
            "5346     Humanoid-v5       TD3   146          43.779619  1.765288e+09\n",
            "5389     Humanoid-v5       TD3   146          52.409301  1.765304e+09\n",
            "5427     Humanoid-v5       TD3   199         204.290074  1.765332e+09\n",
            "5472     Humanoid-v5       TD3   199         214.872181  1.765375e+09\n",
            "5426     Humanoid-v5       TD3   200         106.071675  1.765332e+09\n",
            "5469     Humanoid-v5       TD3   200         106.224323  1.765373e+09\n",
            "728      Walker2d-v5      DDPG     8        1720.564008  1.764639e+09\n",
            "758      Walker2d-v5      DDPG     8        2054.677742  1.764642e+09\n",
            "1043     Walker2d-v5      DDPG     8        2054.677742  1.764642e+09\n",
            "3556     Walker2d-v5      DDPG   182        2181.913794  1.765028e+09\n",
            "3733     Walker2d-v5      DDPG   182        3361.140436  1.765035e+09\n",
            "3421     Walker2d-v5      DDPG   183        3426.236090  1.765020e+09\n",
            "3732     Walker2d-v5      DDPG   183        2093.906846  1.765035e+09\n",
            "3722     Walker2d-v5       SAC   182        4160.081393  1.765035e+09\n",
            "3800     Walker2d-v5       SAC   182        4661.534736  1.765038e+09\n",
            "\n",
            "Dropping 1434 duplicate rows with same final_return_mean\n",
            "Final returns after deduplication: 4041 entries\n",
            "\n",
            "Unique tasks in final_returns: ['Ant-v5', 'HalfCheetah-v5', 'Hopper-v5', 'Humanoid-v5', 'Walker2d-v5']\n",
            "Unique algorithms in final_returns: ['DDPG', 'PPO', 'SAC', 'TD3']\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Data Loading Functions\n",
        "# ============================================================================\n",
        "\n",
        "def load_learning_curves(learning_curves_csv: str = LEARNING_CURVES_CSV, \n",
        "                        learning_curves_dir: str = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load learning curves from BOTH sources and merge:\n",
        "    1. Single CSV file at learning_curves_csv (some data here)\n",
        "    2. Individual CSV files in learning_curves_dir (rest of the data)\n",
        "    \"\"\"\n",
        "    if learning_curves_dir is None:\n",
        "        learning_curves_dir = os.path.join(BASE_DIR, \"learning_curves\")\n",
        "    \n",
        "    all_curves = []\n",
        "    \n",
        "    # Load from single CSV file first\n",
        "    if os.path.exists(learning_curves_csv):\n",
        "        print(f\"Loading from single CSV: {learning_curves_csv}\")\n",
        "        df_single = pd.read_csv(learning_curves_csv, on_bad_lines=\"skip\")\n",
        "        print(f\"  Loaded {len(df_single)} entries\")\n",
        "        all_curves.append(df_single)\n",
        "    \n",
        "    # Load from individual CSV files\n",
        "    if os.path.exists(learning_curves_dir):\n",
        "        csv_files = glob.glob(os.path.join(learning_curves_dir, \"*.csv\"))\n",
        "        if csv_files:\n",
        "            print(f\"Loading from {len(csv_files)} individual files in {learning_curves_dir}\")\n",
        "            individual_curves = []\n",
        "            for csv_file in csv_files:\n",
        "                try:\n",
        "                    filename = os.path.basename(csv_file)\n",
        "                    match = re.match(r\"(.+?)-v5_(.+?)_seed(\\d+)\\.csv\", filename)\n",
        "                    if match:\n",
        "                        task = match.group(1) + \"-v5\"\n",
        "                        algo = match.group(2)\n",
        "                        seed = int(match.group(3))\n",
        "                        df = pd.read_csv(csv_file)\n",
        "                        if all(col in df.columns for col in [\"env_steps\", \"eval_return_mean\"]):\n",
        "                            if \"task\" not in df.columns:\n",
        "                                df[\"task\"] = task\n",
        "                            if \"algorithm\" not in df.columns:\n",
        "                                df[\"algorithm\"] = algo\n",
        "                            if \"seed\" not in df.columns:\n",
        "                                df[\"seed\"] = seed\n",
        "                            individual_curves.append(df)\n",
        "                except:\n",
        "                    continue\n",
        "            if individual_curves:\n",
        "                df_individual = pd.concat(individual_curves, ignore_index=True)\n",
        "                print(f\"  Loaded {len(df_individual)} entries from individual files\")\n",
        "                all_curves.append(df_individual)\n",
        "    \n",
        "    if all_curves:\n",
        "        combined = pd.concat(all_curves, ignore_index=True)\n",
        "        combined = combined.drop_duplicates(\n",
        "            subset=[\"task\", \"algorithm\", \"seed\", \"env_steps\"], keep=\"last\"\n",
        "        ).reset_index(drop=True)\n",
        "        # Drop rows with NaN in key columns\n",
        "        combined = combined.dropna(subset=[\"task\", \"algorithm\", \"seed\", \"env_steps\"])\n",
        "        print(f\"Total: {len(combined)} entries after cleaning\")\n",
        "        return combined\n",
        "    \n",
        "    return pd.DataFrame()\n",
        "\n",
        "\n",
        "def curves_to_rl_stats_format(curves_df: pd.DataFrame, task: str, algo: str):\n",
        "    \"\"\"\n",
        "    Convert learning curves to rl_stats format: (n_steps, n_seeds).\n",
        "    Returns: (data_array, steps_array, seeds_array)\n",
        "    \"\"\"\n",
        "    df = curves_df[(curves_df['task'] == task) & (curves_df['algorithm'] == algo)].copy()\n",
        "    if df.empty:\n",
        "        return np.array([]).reshape(0, 0), np.array([]), np.array([])\n",
        "    \n",
        "    seeds = sorted(df['seed'].unique())\n",
        "    steps = sorted(df['env_steps'].unique())\n",
        "    \n",
        "    data = np.full((len(steps), len(seeds)), np.nan)\n",
        "    for i, step in enumerate(steps):\n",
        "        for j, seed in enumerate(seeds):\n",
        "            val = df[(df['env_steps'] == step) & (df['seed'] == seed)]['eval_return_mean']\n",
        "            if len(val) > 0:\n",
        "                data[i, j] = val.iloc[0]\n",
        "    \n",
        "    return data, np.array(steps), np.array(seeds)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Load learning curves from both sources\n",
        "# ============================================================================\n",
        "curves = load_learning_curves(LEARNING_CURVES_CSV)\n",
        "print(f\"\\nLoaded learning curves shape: {curves.shape}\")\n",
        "if len(curves) > 0:\n",
        "    print(f\"Unique tasks: {sorted(curves.task.dropna().unique())}\")\n",
        "    print(f\"Unique algorithms: {sorted(curves.algorithm.dropna().unique())}\")\n",
        "    print(f\"Total (task, algo, seed) combinations: {len(curves.groupby(['task', 'algorithm', 'seed']))}\")\n",
        "    \n",
        "    # Check for duplicate (task, algorithm, seed, env_steps) entries in curves\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Checking for duplicate (task, algorithm, seed, env_steps) entries in curves...\")\n",
        "    duplicates_curves = curves.groupby(['task', 'algorithm', 'seed', 'env_steps']).filter(lambda x: len(x) > 1)\n",
        "    if len(duplicates_curves) > 0:\n",
        "        print(f\"Found {len(duplicates_curves)} rows with duplicate (task, algorithm, seed, env_steps)\")\n",
        "        \n",
        "        different_curves = []\n",
        "        same_curves_to_drop = []\n",
        "        \n",
        "        for (task, algo, seed, step), group in curves.groupby(['task', 'algorithm', 'seed', 'env_steps']):\n",
        "            if len(group) > 1:\n",
        "                unique_returns = group['eval_return_mean'].nunique()\n",
        "                if unique_returns == 1:\n",
        "                    # Same eval_return_mean - keep first, mark rest for dropping\n",
        "                    same_curves_to_drop.extend(group.index[1:].tolist())\n",
        "                else:\n",
        "                    # Different eval_return_mean - print them\n",
        "                    different_curves.append(group)\n",
        "        \n",
        "        if different_curves:\n",
        "            print(f\"\\n*** WARNING: {len(different_curves)} (task, algo, seed, env_steps) groups have DIFFERENT eval_return_mean values: ***\")\n",
        "            diff_df = pd.concat(different_curves)\n",
        "            display_cols = ['task', 'algorithm', 'seed', 'env_steps', 'eval_return_mean']\n",
        "            if 'timestamp' in diff_df.columns:\n",
        "                display_cols.append('timestamp')\n",
        "            print(diff_df[display_cols].to_string())\n",
        "        \n",
        "        if same_curves_to_drop:\n",
        "            print(f\"\\nDropping {len(same_curves_to_drop)} duplicate rows with same eval_return_mean\")\n",
        "            curves = curves.drop(same_curves_to_drop).reset_index(drop=True)\n",
        "            print(f\"Curves after deduplication: {len(curves)} entries\")\n",
        "    else:\n",
        "        print(\"No duplicate (task, algorithm, seed, env_steps) entries found in curves\")\n",
        "\n",
        "# ============================================================================\n",
        "# Load final evaluation returns (for statistical tests)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Loading final evaluation returns...\")\n",
        "final_returns = pd.read_csv(RESULTS_CSV)\n",
        "print(f\"Loaded {len(final_returns)} entries\")\n",
        "\n",
        "# Check for duplicate (task, algorithm, seed) triples\n",
        "duplicates = final_returns.groupby(['task', 'algorithm', 'seed']).filter(lambda x: len(x) > 1)\n",
        "if len(duplicates) > 0:\n",
        "    print(f\"\\nFound {len(duplicates)} rows with duplicate (task, algorithm, seed) triples\")\n",
        "    \n",
        "    # Check if duplicates have same or different eval_return_mean\n",
        "    different_returns = []\n",
        "    same_returns_to_drop = []\n",
        "    \n",
        "    for (task, algo, seed), group in final_returns.groupby(['task', 'algorithm', 'seed']):\n",
        "        if len(group) > 1:\n",
        "            unique_returns = group['final_return_mean'].nunique()\n",
        "            if unique_returns == 1:\n",
        "                # Same return values - keep first, mark rest for dropping\n",
        "                same_returns_to_drop.extend(group.index[1:].tolist())\n",
        "            else:\n",
        "                # Different return values - print them\n",
        "                different_returns.append(group)\n",
        "    \n",
        "    if different_returns:\n",
        "        print(f\"\\n*** WARNING: {len(different_returns)} (task, algo, seed) groups have DIFFERENT final_return_mean values: ***\")\n",
        "        diff_df = pd.concat(different_returns)\n",
        "        display_cols = ['task', 'algorithm', 'seed', 'final_return_mean']\n",
        "        if 'timestamp' in diff_df.columns:\n",
        "            display_cols.append('timestamp')\n",
        "        print(diff_df[display_cols].to_string())\n",
        "    \n",
        "    if same_returns_to_drop:\n",
        "        print(f\"\\nDropping {len(same_returns_to_drop)} duplicate rows with same final_return_mean\")\n",
        "        final_returns = final_returns.drop(same_returns_to_drop).reset_index(drop=True)\n",
        "        print(f\"Final returns after deduplication: {len(final_returns)} entries\")\n",
        "else:\n",
        "    print(\"No duplicate (task, algorithm, seed) triples found\")\n",
        "\n",
        "print(f\"\\nUnique tasks in final_returns: {sorted(final_returns.task.dropna().unique())}\")\n",
        "print(f\"Unique algorithms in final_returns: {sorted(final_returns.algorithm.dropna().unique())}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7983308",
      "metadata": {},
      "source": [
        "## empirical effect sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8f300030",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Hopper-v5:\n",
            "  SAC vs TD3: ε = 0.185 (Δμ = 152.69, σ_pool = 825.58)\n",
            "  SAC vs DDPG: ε = 1.613 (Δμ = 1321.27, σ_pool = 819.06)\n",
            "  SAC vs PPO: ε = 0.379 (Δμ = 313.69, σ_pool = 828.18)\n",
            "  TD3 vs DDPG: ε = 1.618 (Δμ = 1473.97, σ_pool = 910.96)\n",
            "  TD3 vs PPO: ε = 0.507 (Δμ = 466.39, σ_pool = 919.63)\n",
            "  DDPG vs PPO: ε = 1.103 (Δμ = 1007.58, σ_pool = 913.32)\n",
            "\n",
            "Walker2d-v5:\n",
            "  SAC vs TD3: ε = 0.177 (Δμ = 117.61, σ_pool = 664.03)\n",
            "  SAC vs DDPG: ε = 2.193 (Δμ = 1855.16, σ_pool = 845.87)\n",
            "  SAC vs PPO: ε = 2.851 (Δμ = 2098.01, σ_pool = 735.89)\n",
            "  TD3 vs DDPG: ε = 1.842 (Δμ = 1737.55, σ_pool = 943.38)\n",
            "  TD3 vs PPO: ε = 2.338 (Δμ = 1980.40, σ_pool = 846.95)\n",
            "  DDPG vs PPO: ε = 0.244 (Δμ = 242.85, σ_pool = 994.90)\n",
            "\n",
            "HalfCheetah-v5:\n",
            "  SAC vs TD3: ε = 0.561 (Δμ = 2368.51, σ_pool = 4218.94)\n",
            "  SAC vs DDPG: ε = 0.761 (Δμ = 2775.11, σ_pool = 3644.77)\n",
            "  SAC vs PPO: ε = 2.613 (Δμ = 8356.77, σ_pool = 3197.58)\n",
            "  TD3 vs DDPG: ε = 0.120 (Δμ = 406.60, σ_pool = 3382.05)\n",
            "  TD3 vs PPO: ε = 2.070 (Δμ = 5988.26, σ_pool = 2892.95)\n",
            "  DDPG vs PPO: ε = 2.839 (Δμ = 5581.65, σ_pool = 1966.31)\n",
            "\n",
            "Ant-v5:\n",
            "  SAC vs TD3: ε = 1.792 (Δμ = 1706.36, σ_pool = 952.31)\n",
            "  SAC vs DDPG: ε = 5.284 (Δμ = 5358.02, σ_pool = 1013.97)\n",
            "  SAC vs PPO: ε = 4.520 (Δμ = 4304.84, σ_pool = 952.42)\n",
            "  TD3 vs DDPG: ε = 4.480 (Δμ = 3651.66, σ_pool = 815.11)\n",
            "  TD3 vs PPO: ε = 3.531 (Δμ = 2598.48, σ_pool = 735.90)\n",
            "  DDPG vs PPO: ε = 1.292 (Δμ = 1053.18, σ_pool = 815.24)\n",
            "\n",
            "Humanoid-v5:\n",
            "  SAC vs TD3: ε = 16.299 (Δμ = 6836.07, σ_pool = 419.42)\n",
            "  SAC vs DDPG: ε = 16.298 (Δμ = 6849.51, σ_pool = 420.26)\n",
            "  SAC vs PPO: ε = 15.196 (Δμ = 6394.27, σ_pool = 420.78)\n",
            "  TD3 vs DDPG: ε = 0.170 (Δμ = 13.44, σ_pool = 79.17)\n",
            "  TD3 vs PPO: ε = 5.769 (Δμ = 441.79, σ_pool = 76.58)\n",
            "  DDPG vs PPO: ε = 6.013 (Δμ = 455.24, σ_pool = 75.70)\n",
            "\n",
            "============================================================\n",
            "Summary of Empirical Effect Sizes\n",
            "============================================================\n",
            "Mean empirical effect size: 3.489\n",
            "Median empirical effect size: 1.956\n",
            "Min: 0.120, Max: 16.299\n",
            "\n",
            "Effect sizes by task:\n",
            "  Hopper-v5: mean ε = 0.901\n",
            "  Walker2d-v5: mean ε = 1.608\n",
            "  HalfCheetah-v5: mean ε = 1.494\n",
            "  Ant-v5: mean ε = 3.483\n",
            "  Humanoid-v5: mean ε = 9.958\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>task</th>\n",
              "      <th>algo1</th>\n",
              "      <th>algo2</th>\n",
              "      <th>mean1</th>\n",
              "      <th>mean2</th>\n",
              "      <th>delta_mu</th>\n",
              "      <th>sigma_pool</th>\n",
              "      <th>epsilon_empirical</th>\n",
              "      <th>n1</th>\n",
              "      <th>n2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hopper-v5</td>\n",
              "      <td>SAC</td>\n",
              "      <td>TD3</td>\n",
              "      <td>2931.936823</td>\n",
              "      <td>3084.631793</td>\n",
              "      <td>152.694970</td>\n",
              "      <td>825.580198</td>\n",
              "      <td>0.184955</td>\n",
              "      <td>202</td>\n",
              "      <td>201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hopper-v5</td>\n",
              "      <td>SAC</td>\n",
              "      <td>DDPG</td>\n",
              "      <td>2931.936823</td>\n",
              "      <td>1610.662780</td>\n",
              "      <td>1321.274043</td>\n",
              "      <td>819.060788</td>\n",
              "      <td>1.613157</td>\n",
              "      <td>202</td>\n",
              "      <td>203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hopper-v5</td>\n",
              "      <td>SAC</td>\n",
              "      <td>PPO</td>\n",
              "      <td>2931.936823</td>\n",
              "      <td>2618.242535</td>\n",
              "      <td>313.694288</td>\n",
              "      <td>828.184472</td>\n",
              "      <td>0.378773</td>\n",
              "      <td>202</td>\n",
              "      <td>201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hopper-v5</td>\n",
              "      <td>TD3</td>\n",
              "      <td>DDPG</td>\n",
              "      <td>3084.631793</td>\n",
              "      <td>1610.662780</td>\n",
              "      <td>1473.969013</td>\n",
              "      <td>910.960723</td>\n",
              "      <td>1.618038</td>\n",
              "      <td>201</td>\n",
              "      <td>203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hopper-v5</td>\n",
              "      <td>TD3</td>\n",
              "      <td>PPO</td>\n",
              "      <td>3084.631793</td>\n",
              "      <td>2618.242535</td>\n",
              "      <td>466.389258</td>\n",
              "      <td>919.625414</td>\n",
              "      <td>0.507151</td>\n",
              "      <td>201</td>\n",
              "      <td>201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Hopper-v5</td>\n",
              "      <td>DDPG</td>\n",
              "      <td>PPO</td>\n",
              "      <td>1610.662780</td>\n",
              "      <td>2618.242535</td>\n",
              "      <td>1007.579755</td>\n",
              "      <td>913.315708</td>\n",
              "      <td>1.103211</td>\n",
              "      <td>203</td>\n",
              "      <td>201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Walker2d-v5</td>\n",
              "      <td>SAC</td>\n",
              "      <td>TD3</td>\n",
              "      <td>4426.519844</td>\n",
              "      <td>4308.910441</td>\n",
              "      <td>117.609403</td>\n",
              "      <td>664.032289</td>\n",
              "      <td>0.177114</td>\n",
              "      <td>202</td>\n",
              "      <td>201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Walker2d-v5</td>\n",
              "      <td>SAC</td>\n",
              "      <td>DDPG</td>\n",
              "      <td>4426.519844</td>\n",
              "      <td>2571.364218</td>\n",
              "      <td>1855.155626</td>\n",
              "      <td>845.871240</td>\n",
              "      <td>2.193189</td>\n",
              "      <td>202</td>\n",
              "      <td>205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Walker2d-v5</td>\n",
              "      <td>SAC</td>\n",
              "      <td>PPO</td>\n",
              "      <td>4426.519844</td>\n",
              "      <td>2328.510149</td>\n",
              "      <td>2098.009695</td>\n",
              "      <td>735.894322</td>\n",
              "      <td>2.850966</td>\n",
              "      <td>202</td>\n",
              "      <td>201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Walker2d-v5</td>\n",
              "      <td>TD3</td>\n",
              "      <td>DDPG</td>\n",
              "      <td>4308.910441</td>\n",
              "      <td>2571.364218</td>\n",
              "      <td>1737.546223</td>\n",
              "      <td>943.379080</td>\n",
              "      <td>1.841832</td>\n",
              "      <td>201</td>\n",
              "      <td>205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Walker2d-v5</td>\n",
              "      <td>TD3</td>\n",
              "      <td>PPO</td>\n",
              "      <td>4308.910441</td>\n",
              "      <td>2328.510149</td>\n",
              "      <td>1980.400292</td>\n",
              "      <td>846.945189</td>\n",
              "      <td>2.338286</td>\n",
              "      <td>201</td>\n",
              "      <td>201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Walker2d-v5</td>\n",
              "      <td>DDPG</td>\n",
              "      <td>PPO</td>\n",
              "      <td>2571.364218</td>\n",
              "      <td>2328.510149</td>\n",
              "      <td>242.854068</td>\n",
              "      <td>994.896286</td>\n",
              "      <td>0.244100</td>\n",
              "      <td>205</td>\n",
              "      <td>201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>HalfCheetah-v5</td>\n",
              "      <td>SAC</td>\n",
              "      <td>TD3</td>\n",
              "      <td>10347.633069</td>\n",
              "      <td>7979.122916</td>\n",
              "      <td>2368.510153</td>\n",
              "      <td>4218.939833</td>\n",
              "      <td>0.561399</td>\n",
              "      <td>201</td>\n",
              "      <td>201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>HalfCheetah-v5</td>\n",
              "      <td>SAC</td>\n",
              "      <td>DDPG</td>\n",
              "      <td>10347.633069</td>\n",
              "      <td>7572.520345</td>\n",
              "      <td>2775.112725</td>\n",
              "      <td>3644.774034</td>\n",
              "      <td>0.761395</td>\n",
              "      <td>201</td>\n",
              "      <td>202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>HalfCheetah-v5</td>\n",
              "      <td>SAC</td>\n",
              "      <td>PPO</td>\n",
              "      <td>10347.633069</td>\n",
              "      <td>1990.865500</td>\n",
              "      <td>8356.767570</td>\n",
              "      <td>3197.583653</td>\n",
              "      <td>2.613463</td>\n",
              "      <td>201</td>\n",
              "      <td>200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>HalfCheetah-v5</td>\n",
              "      <td>TD3</td>\n",
              "      <td>DDPG</td>\n",
              "      <td>7979.122916</td>\n",
              "      <td>7572.520345</td>\n",
              "      <td>406.602571</td>\n",
              "      <td>3382.050910</td>\n",
              "      <td>0.120224</td>\n",
              "      <td>201</td>\n",
              "      <td>202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>HalfCheetah-v5</td>\n",
              "      <td>TD3</td>\n",
              "      <td>PPO</td>\n",
              "      <td>7979.122916</td>\n",
              "      <td>1990.865500</td>\n",
              "      <td>5988.257416</td>\n",
              "      <td>2892.953186</td>\n",
              "      <td>2.069946</td>\n",
              "      <td>201</td>\n",
              "      <td>200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>HalfCheetah-v5</td>\n",
              "      <td>DDPG</td>\n",
              "      <td>PPO</td>\n",
              "      <td>7572.520345</td>\n",
              "      <td>1990.865500</td>\n",
              "      <td>5581.654845</td>\n",
              "      <td>1966.313957</td>\n",
              "      <td>2.838639</td>\n",
              "      <td>202</td>\n",
              "      <td>200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Ant-v5</td>\n",
              "      <td>SAC</td>\n",
              "      <td>TD3</td>\n",
              "      <td>5846.539783</td>\n",
              "      <td>4140.177053</td>\n",
              "      <td>1706.362730</td>\n",
              "      <td>952.305368</td>\n",
              "      <td>1.791823</td>\n",
              "      <td>201</td>\n",
              "      <td>201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Ant-v5</td>\n",
              "      <td>SAC</td>\n",
              "      <td>DDPG</td>\n",
              "      <td>5846.539783</td>\n",
              "      <td>488.516502</td>\n",
              "      <td>5358.023280</td>\n",
              "      <td>1013.969889</td>\n",
              "      <td>5.284204</td>\n",
              "      <td>201</td>\n",
              "      <td>203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Ant-v5</td>\n",
              "      <td>SAC</td>\n",
              "      <td>PPO</td>\n",
              "      <td>5846.539783</td>\n",
              "      <td>1541.697027</td>\n",
              "      <td>4304.842756</td>\n",
              "      <td>952.420979</td>\n",
              "      <td>4.519895</td>\n",
              "      <td>201</td>\n",
              "      <td>201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Ant-v5</td>\n",
              "      <td>TD3</td>\n",
              "      <td>DDPG</td>\n",
              "      <td>4140.177053</td>\n",
              "      <td>488.516502</td>\n",
              "      <td>3651.660551</td>\n",
              "      <td>815.106476</td>\n",
              "      <td>4.479980</td>\n",
              "      <td>201</td>\n",
              "      <td>203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Ant-v5</td>\n",
              "      <td>TD3</td>\n",
              "      <td>PPO</td>\n",
              "      <td>4140.177053</td>\n",
              "      <td>1541.697027</td>\n",
              "      <td>2598.480026</td>\n",
              "      <td>735.901265</td>\n",
              "      <td>3.531017</td>\n",
              "      <td>201</td>\n",
              "      <td>201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Ant-v5</td>\n",
              "      <td>DDPG</td>\n",
              "      <td>PPO</td>\n",
              "      <td>488.516502</td>\n",
              "      <td>1541.697027</td>\n",
              "      <td>1053.180525</td>\n",
              "      <td>815.240871</td>\n",
              "      <td>1.291864</td>\n",
              "      <td>203</td>\n",
              "      <td>201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Humanoid-v5</td>\n",
              "      <td>SAC</td>\n",
              "      <td>TD3</td>\n",
              "      <td>6951.219144</td>\n",
              "      <td>115.153573</td>\n",
              "      <td>6836.065571</td>\n",
              "      <td>419.418586</td>\n",
              "      <td>16.298909</td>\n",
              "      <td>203</td>\n",
              "      <td>206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>Humanoid-v5</td>\n",
              "      <td>SAC</td>\n",
              "      <td>DDPG</td>\n",
              "      <td>6951.219144</td>\n",
              "      <td>101.708958</td>\n",
              "      <td>6849.510185</td>\n",
              "      <td>420.260179</td>\n",
              "      <td>16.298261</td>\n",
              "      <td>203</td>\n",
              "      <td>204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>Humanoid-v5</td>\n",
              "      <td>SAC</td>\n",
              "      <td>PPO</td>\n",
              "      <td>6951.219144</td>\n",
              "      <td>556.948144</td>\n",
              "      <td>6394.271000</td>\n",
              "      <td>420.781995</td>\n",
              "      <td>15.196161</td>\n",
              "      <td>203</td>\n",
              "      <td>202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Humanoid-v5</td>\n",
              "      <td>TD3</td>\n",
              "      <td>DDPG</td>\n",
              "      <td>115.153573</td>\n",
              "      <td>101.708958</td>\n",
              "      <td>13.444614</td>\n",
              "      <td>79.170060</td>\n",
              "      <td>0.169819</td>\n",
              "      <td>206</td>\n",
              "      <td>204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Humanoid-v5</td>\n",
              "      <td>TD3</td>\n",
              "      <td>PPO</td>\n",
              "      <td>115.153573</td>\n",
              "      <td>556.948144</td>\n",
              "      <td>441.794571</td>\n",
              "      <td>76.579100</td>\n",
              "      <td>5.769127</td>\n",
              "      <td>206</td>\n",
              "      <td>202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Humanoid-v5</td>\n",
              "      <td>DDPG</td>\n",
              "      <td>PPO</td>\n",
              "      <td>101.708958</td>\n",
              "      <td>556.948144</td>\n",
              "      <td>455.239185</td>\n",
              "      <td>75.703074</td>\n",
              "      <td>6.013484</td>\n",
              "      <td>204</td>\n",
              "      <td>202</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              task algo1 algo2         mean1        mean2     delta_mu  \\\n",
              "0        Hopper-v5   SAC   TD3   2931.936823  3084.631793   152.694970   \n",
              "1        Hopper-v5   SAC  DDPG   2931.936823  1610.662780  1321.274043   \n",
              "2        Hopper-v5   SAC   PPO   2931.936823  2618.242535   313.694288   \n",
              "3        Hopper-v5   TD3  DDPG   3084.631793  1610.662780  1473.969013   \n",
              "4        Hopper-v5   TD3   PPO   3084.631793  2618.242535   466.389258   \n",
              "5        Hopper-v5  DDPG   PPO   1610.662780  2618.242535  1007.579755   \n",
              "6      Walker2d-v5   SAC   TD3   4426.519844  4308.910441   117.609403   \n",
              "7      Walker2d-v5   SAC  DDPG   4426.519844  2571.364218  1855.155626   \n",
              "8      Walker2d-v5   SAC   PPO   4426.519844  2328.510149  2098.009695   \n",
              "9      Walker2d-v5   TD3  DDPG   4308.910441  2571.364218  1737.546223   \n",
              "10     Walker2d-v5   TD3   PPO   4308.910441  2328.510149  1980.400292   \n",
              "11     Walker2d-v5  DDPG   PPO   2571.364218  2328.510149   242.854068   \n",
              "12  HalfCheetah-v5   SAC   TD3  10347.633069  7979.122916  2368.510153   \n",
              "13  HalfCheetah-v5   SAC  DDPG  10347.633069  7572.520345  2775.112725   \n",
              "14  HalfCheetah-v5   SAC   PPO  10347.633069  1990.865500  8356.767570   \n",
              "15  HalfCheetah-v5   TD3  DDPG   7979.122916  7572.520345   406.602571   \n",
              "16  HalfCheetah-v5   TD3   PPO   7979.122916  1990.865500  5988.257416   \n",
              "17  HalfCheetah-v5  DDPG   PPO   7572.520345  1990.865500  5581.654845   \n",
              "18          Ant-v5   SAC   TD3   5846.539783  4140.177053  1706.362730   \n",
              "19          Ant-v5   SAC  DDPG   5846.539783   488.516502  5358.023280   \n",
              "20          Ant-v5   SAC   PPO   5846.539783  1541.697027  4304.842756   \n",
              "21          Ant-v5   TD3  DDPG   4140.177053   488.516502  3651.660551   \n",
              "22          Ant-v5   TD3   PPO   4140.177053  1541.697027  2598.480026   \n",
              "23          Ant-v5  DDPG   PPO    488.516502  1541.697027  1053.180525   \n",
              "24     Humanoid-v5   SAC   TD3   6951.219144   115.153573  6836.065571   \n",
              "25     Humanoid-v5   SAC  DDPG   6951.219144   101.708958  6849.510185   \n",
              "26     Humanoid-v5   SAC   PPO   6951.219144   556.948144  6394.271000   \n",
              "27     Humanoid-v5   TD3  DDPG    115.153573   101.708958    13.444614   \n",
              "28     Humanoid-v5   TD3   PPO    115.153573   556.948144   441.794571   \n",
              "29     Humanoid-v5  DDPG   PPO    101.708958   556.948144   455.239185   \n",
              "\n",
              "     sigma_pool  epsilon_empirical   n1   n2  \n",
              "0    825.580198           0.184955  202  201  \n",
              "1    819.060788           1.613157  202  203  \n",
              "2    828.184472           0.378773  202  201  \n",
              "3    910.960723           1.618038  201  203  \n",
              "4    919.625414           0.507151  201  201  \n",
              "5    913.315708           1.103211  203  201  \n",
              "6    664.032289           0.177114  202  201  \n",
              "7    845.871240           2.193189  202  205  \n",
              "8    735.894322           2.850966  202  201  \n",
              "9    943.379080           1.841832  201  205  \n",
              "10   846.945189           2.338286  201  201  \n",
              "11   994.896286           0.244100  205  201  \n",
              "12  4218.939833           0.561399  201  201  \n",
              "13  3644.774034           0.761395  201  202  \n",
              "14  3197.583653           2.613463  201  200  \n",
              "15  3382.050910           0.120224  201  202  \n",
              "16  2892.953186           2.069946  201  200  \n",
              "17  1966.313957           2.838639  202  200  \n",
              "18   952.305368           1.791823  201  201  \n",
              "19  1013.969889           5.284204  201  203  \n",
              "20   952.420979           4.519895  201  201  \n",
              "21   815.106476           4.479980  201  203  \n",
              "22   735.901265           3.531017  201  201  \n",
              "23   815.240871           1.291864  203  201  \n",
              "24   419.418586          16.298909  203  206  \n",
              "25   420.260179          16.298261  203  204  \n",
              "26   420.781995          15.196161  203  202  \n",
              "27    79.170060           0.169819  206  204  \n",
              "28    76.579100           5.769127  206  202  \n",
              "29    75.703074           6.013484  204  202  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "empirical_effect_sizes = []\n",
        "\n",
        "for task in TASKS:\n",
        "    task_df = final_returns[final_returns['task'] == task]\n",
        "    available_algos = [a for a in ALGORITHMS if a in task_df['algorithm'].unique()]\n",
        "    \n",
        "    if len(available_algos) < 2:\n",
        "        continue\n",
        "    \n",
        "    print(f\"\\n{task}:\")\n",
        "    \n",
        "    # Compute effect size for all algorithm pairs\n",
        "    for i, algo1 in enumerate(available_algos):\n",
        "        for algo2 in available_algos[i+1:]:\n",
        "            sample1 = task_df[task_df['algorithm'] == algo1]['final_return_mean'].values\n",
        "            sample2 = task_df[task_df['algorithm'] == algo2]['final_return_mean'].values\n",
        "            \n",
        "            if len(sample1) < 2 or len(sample2) < 2:\n",
        "                continue\n",
        "            \n",
        "            # Compute pooled standard deviation\n",
        "            mean1, mean2 = np.mean(sample1), np.mean(sample2)\n",
        "            std1, std2 = np.std(sample1, ddof=1), np.std(sample2, ddof=1)\n",
        "            n1, n2 = len(sample1), len(sample2)\n",
        "            sigma_pool = np.sqrt(((n1-1)*std1**2 + (n2-1)*std2**2) / (n1 + n2 - 2))\n",
        "            \n",
        "            # Empirical effect size: ε = |Δμ| / σ_pool\n",
        "            delta_mu = abs(mean1 - mean2)\n",
        "            epsilon_empirical = delta_mu / sigma_pool if sigma_pool > 0 else 0\n",
        "            \n",
        "            empirical_effect_sizes.append({\n",
        "                'task': task,\n",
        "                'algo1': algo1,\n",
        "                'algo2': algo2,\n",
        "                'mean1': mean1,\n",
        "                'mean2': mean2,\n",
        "                'delta_mu': delta_mu,\n",
        "                'sigma_pool': sigma_pool,\n",
        "                'epsilon_empirical': epsilon_empirical,\n",
        "                'n1': n1,\n",
        "                'n2': n2\n",
        "            })\n",
        "            \n",
        "            print(f\"  {algo1} vs {algo2}: ε = {epsilon_empirical:.3f} (Δμ = {delta_mu:.2f}, σ_pool = {sigma_pool:.2f})\")\n",
        "\n",
        "empirical_effects_df = pd.DataFrame(empirical_effect_sizes)\n",
        "\n",
        "if len(empirical_effects_df) > 0:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Summary of Empirical Effect Sizes\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Mean empirical effect size: {empirical_effects_df['epsilon_empirical'].mean():.3f}\")\n",
        "    print(f\"Median empirical effect size: {empirical_effects_df['epsilon_empirical'].median():.3f}\")\n",
        "    print(f\"Min: {empirical_effects_df['epsilon_empirical'].min():.3f}, Max: {empirical_effects_df['epsilon_empirical'].max():.3f}\")\n",
        "    print(\"\\nEffect sizes by task:\")\n",
        "    for task in TASKS:\n",
        "        task_effects = empirical_effects_df[empirical_effects_df['task'] == task]\n",
        "        if len(task_effects) > 0:\n",
        "            print(f\"  {task}: mean ε = {task_effects['epsilon_empirical'].mean():.3f}\")\n",
        "    \n",
        "    display(empirical_effects_df)\n",
        "else:\n",
        "    print(\"No effect sizes computed. Make sure final_returns is loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a81b2ee",
      "metadata": {},
      "source": [
        "## power aggregated plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "02713385",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Loading power analysis results from all tasks\n",
            "============================================================\n",
            "Loading Hopper-v5...\n",
            "  Loaded 1728 rows\n",
            "Loading Walker2d-v5...\n",
            "  Loaded 1728 rows\n",
            "Loading HalfCheetah-v5...\n",
            "  Loaded 1728 rows\n",
            "Loading Ant-v5...\n",
            "  Loaded 1728 rows\n",
            "Loading Humanoid-v5...\n",
            "  Loaded 1728 rows\n",
            "\n",
            "Total power results: 8640 rows\n",
            "Tasks: ['Ant-v5', 'HalfCheetah-v5', 'Hopper-v5', 'Humanoid-v5', 'Walker2d-v5']\n",
            "Tests: ['Mann-Whitney', 'Ranked t-test', 'Welch t-test', 'bootstrap', 'permutation', 't-test']\n",
            "Alphas: [0.01, 0.05]\n",
            "Epsilons: [0.5, 1.0, 2.0]\n"
          ]
        }
      ],
      "source": [
        "power_path = \"/n/home09/annabelma/rl_final_proj/power_experiments/results/12_10_final\"\n",
        "\n",
        "# ============================================================================\n",
        "# Load Power Analysis Results from All Tasks\n",
        "# ============================================================================\n",
        "print(\"=\"*60)\n",
        "print(\"Loading power analysis results from all tasks\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "power_dataframes = []\n",
        "tasks_dir = os.path.join(power_path, \"tasks\")\n",
        "\n",
        "for task in TASKS:\n",
        "    # Convert task name to directory format (e.g., \"Ant-v5\" -> \"Ant_v5\")\n",
        "    task_dir_name = task.replace('-', '_')\n",
        "    task_csv = os.path.join(tasks_dir, task_dir_name, f\"{task_dir_name}_power_empirical_df.csv\")\n",
        "    \n",
        "    if os.path.exists(task_csv):\n",
        "        print(f\"Loading {task}...\")\n",
        "        df = pd.read_csv(task_csv)\n",
        "        power_dataframes.append(df)\n",
        "        print(f\"  Loaded {len(df)} rows\")\n",
        "    else:\n",
        "        print(f\"  Warning: {task_csv} not found\")\n",
        "\n",
        "if power_dataframes:\n",
        "    power_empirical_df = pd.concat(power_dataframes, ignore_index=True)\n",
        "    print(f\"\\nTotal power results: {len(power_empirical_df)} rows\")\n",
        "    print(f\"Tasks: {sorted(power_empirical_df['task'].unique())}\")\n",
        "    print(f\"Tests: {sorted(power_empirical_df['test'].unique())}\")\n",
        "    print(f\"Alphas: {sorted(power_empirical_df['alpha'].unique())}\")\n",
        "    print(f\"Epsilons: {sorted(power_empirical_df['epsilon'].unique())}\")\n",
        "else:\n",
        "    print(\"No power data found!\")\n",
        "    power_empirical_df = pd.DataFrame() \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "57f79a57",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Generating aggregated power plots (across all tasks)\n",
            "============================================================\n",
            "\n",
            "Aggregating power results across all tasks and algorithm pairs...\n",
            "Aggregated to 288 unique (test, alpha, epsilon, target_n) combinations\n",
            "\n",
            "Aggregated power plot saved to: /n/home09/annabelma/rl_final_proj/power_experiments/results/12_10_final/power_aggregated_all_tasks.png\n",
            "============================================================\n",
            "\n",
            "Average Power by test, effect size, and alpha (aggregated across all tasks):\n",
            "  Mann-Whitney         (ε=0.5, α=0.010): Power = 0.2423 ± 0.0073\n",
            "  Mann-Whitney         (ε=0.5, α=0.050): Power = 0.3476 ± 0.0083\n",
            "  Mann-Whitney         (ε=1.0, α=0.010): Power = 0.4526 ± 0.0065\n",
            "  Mann-Whitney         (ε=1.0, α=0.050): Power = 0.5395 ± 0.0062\n",
            "  Mann-Whitney         (ε=2.0, α=0.010): Power = 0.6281 ± 0.0040\n",
            "  Mann-Whitney         (ε=2.0, α=0.050): Power = 0.6788 ± 0.0031\n",
            "  Ranked t-test        (ε=0.5, α=0.010): Power = 0.2556 ± 0.0077\n",
            "  Ranked t-test        (ε=0.5, α=0.050): Power = 0.3803 ± 0.0101\n",
            "  Ranked t-test        (ε=1.0, α=0.010): Power = 0.4722 ± 0.0068\n",
            "  Ranked t-test        (ε=1.0, α=0.050): Power = 0.5946 ± 0.0081\n",
            "  Ranked t-test        (ε=2.0, α=0.010): Power = 0.6499 ± 0.0038\n",
            "  Ranked t-test        (ε=2.0, α=0.050): Power = 0.7689 ± 0.0046\n",
            "  Welch t-test         (ε=0.5, α=0.010): Power = 0.2214 ± 0.0088\n",
            "  Welch t-test         (ε=0.5, α=0.050): Power = 0.3564 ± 0.0110\n",
            "  Welch t-test         (ε=1.0, α=0.010): Power = 0.4955 ± 0.0073\n",
            "  Welch t-test         (ε=1.0, α=0.050): Power = 0.6078 ± 0.0078\n",
            "  Welch t-test         (ε=2.0, α=0.010): Power = 0.6833 ± 0.0049\n",
            "  Welch t-test         (ε=2.0, α=0.050): Power = 0.7840 ± 0.0053\n",
            "  bootstrap            (ε=0.5, α=0.010): Power = 0.3554 ± 0.0131\n",
            "  bootstrap            (ε=0.5, α=0.050): Power = 0.4809 ± 0.0136\n",
            "  bootstrap            (ε=1.0, α=0.010): Power = 0.6853 ± 0.0105\n",
            "  bootstrap            (ε=1.0, α=0.050): Power = 0.7617 ± 0.0094\n",
            "  bootstrap            (ε=2.0, α=0.010): Power = 0.9305 ± 0.0046\n",
            "  bootstrap            (ε=2.0, α=0.050): Power = 0.9512 ± 0.0038\n",
            "  permutation          (ε=0.5, α=0.010): Power = 0.3045 ± 0.0118\n",
            "  permutation          (ε=0.5, α=0.050): Power = 0.4293 ± 0.0128\n",
            "  permutation          (ε=1.0, α=0.010): Power = 0.6174 ± 0.0102\n",
            "  permutation          (ε=1.0, α=0.050): Power = 0.7069 ± 0.0094\n",
            "  permutation          (ε=2.0, α=0.010): Power = 0.8792 ± 0.0058\n",
            "  permutation          (ε=2.0, α=0.050): Power = 0.9228 ± 0.0047\n",
            "  t-test               (ε=0.5, α=0.010): Power = 0.2315 ± 0.0096\n",
            "  t-test               (ε=0.5, α=0.050): Power = 0.3754 ± 0.0117\n",
            "  t-test               (ε=1.0, α=0.010): Power = 0.5142 ± 0.0080\n",
            "  t-test               (ε=1.0, α=0.050): Power = 0.6319 ± 0.0085\n",
            "  t-test               (ε=2.0, α=0.010): Power = 0.7216 ± 0.0056\n",
            "  t-test               (ε=2.0, α=0.050): Power = 0.8265 ± 0.0055\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Generate Aggregated Power Plots (across all tasks)\n",
        "# Format: 3 rows (epsilon) × 2 columns (alpha)\n",
        "# ============================================================================\n",
        "\n",
        "if len(power_empirical_df) > 0:\n",
        "    print(\"=\"*60)\n",
        "    print(\"Generating aggregated power plots (across all tasks)\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Color map for tests (matching paper style)\n",
        "    test_colors = {\n",
        "        't-test': '#1f77b4',           # blue\n",
        "        'Welch t-test': '#ff7f0e',     # orange\n",
        "        'Mann-Whitney': '#2ca02c',     # green\n",
        "        'Ranked t-test': '#9467bd',    # purple\n",
        "        'bootstrap': '#17becf',        # cyan\n",
        "        'permutation': '#bcbd22'       # yellow-green\n",
        "    }\n",
        "    \n",
        "    tests_list = ['t-test', \"Welch t-test\", 'Mann-Whitney', 'Ranked t-test', 'bootstrap', 'permutation']\n",
        "    \n",
        "    # Alpha and epsilon order\n",
        "    alpha_order = [0.01, 0.05]  # Columns: left to right\n",
        "    epsilon_order = [0.5, 1.0, 2.0]  # Rows: top to bottom\n",
        "    \n",
        "    # Aggregate across all tasks and algorithm pairs\n",
        "    print(\"\\nAggregating power results across all tasks and algorithm pairs...\")\n",
        "    aggregated_data = power_empirical_df.groupby(['test', 'alpha', 'epsilon', 'target_n']).agg({\n",
        "        'power': 'mean',\n",
        "        'se': 'mean'\n",
        "    }).reset_index()\n",
        "    \n",
        "    print(f\"Aggregated to {len(aggregated_data)} unique (test, alpha, epsilon, target_n) combinations\")\n",
        "    \n",
        "    # Create figure with subplots: 3 rows × 2 columns\n",
        "    fig, axes = plt.subplots(3, 2, figsize=(14, 12))\n",
        "    fig.suptitle('Aggregated Power vs Sample Size (across all tasks)\\nAverage across all algorithm pairs', \n",
        "                 fontsize=16, fontweight='bold', y=0.995)\n",
        "    \n",
        "    # Iterate over rows (epsilon) and columns (alpha)\n",
        "    for row_idx, epsilon in enumerate(epsilon_order):\n",
        "        for col_idx, alpha in enumerate(alpha_order):\n",
        "            ax = axes[row_idx, col_idx]\n",
        "            \n",
        "            # Filter data for this (alpha, epsilon) combination\n",
        "            combo_data = aggregated_data[\n",
        "                (aggregated_data['alpha'] == alpha) &\n",
        "                (aggregated_data['epsilon'] == epsilon)\n",
        "            ]\n",
        "            \n",
        "            if len(combo_data) == 0:\n",
        "                ax.text(0.5, 0.5, 'No data', ha='center', va='center', transform=ax.transAxes)\n",
        "                ax.set_title(f'α = {alpha:.2f}, ε = {epsilon:.1f}', fontsize=11)\n",
        "                continue\n",
        "            \n",
        "            # Group by test and target_n\n",
        "            plot_data = combo_data.groupby(['test', 'target_n']).agg({\n",
        "                'power': 'mean',\n",
        "                'se': 'mean'\n",
        "            }).reset_index()\n",
        "            \n",
        "            if len(plot_data) == 0:\n",
        "                ax.text(0.5, 0.5, 'No data', ha='center', va='center', transform=ax.transAxes)\n",
        "                ax.set_title(f'α = {alpha:.2f}, ε = {epsilon:.1f}', fontsize=11)\n",
        "                continue\n",
        "            \n",
        "            # Plot each test\n",
        "            for test_name in tests_list:\n",
        "                test_data = plot_data[plot_data['test'] == test_name].sort_values('target_n')\n",
        "                if len(test_data) > 0:\n",
        "                    color = test_colors.get(test_name, '#000000')\n",
        "                    ax.plot(test_data['target_n'], test_data['power'], \n",
        "                           marker='o', label=test_name, linewidth=1.5, color=color, markersize=4)\n",
        "                    # Add error bars\n",
        "                    ax.errorbar(test_data['target_n'], test_data['power'], \n",
        "                               yerr=test_data['se'], \n",
        "                               fmt='none', color=color, alpha=0.3, capsize=2)\n",
        "            \n",
        "            # Add reference line at 0.8 power (target)\n",
        "            ax.axhline(y=0.8, color='red', linestyle='--', linewidth=1.5, \n",
        "                      alpha=0.7, zorder=0)\n",
        "            \n",
        "            # Formatting\n",
        "            if row_idx == 2:  # Bottom row\n",
        "                ax.set_xlabel('Sample size N (log scale)', fontsize=10)\n",
        "            if col_idx == 0:  # Left column\n",
        "                ax.set_ylabel('Power (1 - β*)', fontsize=10)\n",
        "            \n",
        "            ax.set_title(f'α = {alpha:.2f}, ε = {epsilon:.1f}', fontsize=11, fontweight='bold')\n",
        "            ax.set_xscale('log')\n",
        "            \n",
        "            # Set x-axis ticks\n",
        "            available_n = sorted(plot_data['target_n'].unique())\n",
        "            ax.set_xticks(available_n)\n",
        "            ax.set_xticklabels([str(int(n)) for n in available_n], fontsize=9)\n",
        "            \n",
        "            # Y-axis: show from 0 to 1\n",
        "            ax.set_ylim([0, 1.05])\n",
        "            ax.set_yticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
        "            ax.tick_params(axis='y', labelsize=9)\n",
        "            \n",
        "            ax.grid(True, alpha=0.3, which='both')\n",
        "            \n",
        "            # Add legend only to top-right subplot\n",
        "            if row_idx == 0 and col_idx == 1:\n",
        "                ax.legend(loc='upper left', frameon=True, fontsize=8, ncol=2)\n",
        "    \n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.98])  # Leave space for suptitle\n",
        "    \n",
        "    # Save plot\n",
        "    output_dir = power_path\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    plot_filename = \"power_aggregated_all_tasks.png\"\n",
        "    plot_path = os.path.join(output_dir, plot_filename)\n",
        "    plt.savefig(plot_path, bbox_inches='tight', dpi=150)\n",
        "    plt.close()\n",
        "    \n",
        "    print(f\"\\nAggregated power plot saved to: {plot_path}\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Display summary statistics\n",
        "    print(\"\\nAverage Power by test, effect size, and alpha (aggregated across all tasks):\")\n",
        "    summary = power_empirical_df.groupby(['test', 'epsilon', 'alpha']).agg({\n",
        "        'power': 'mean',\n",
        "        'se': 'mean'\n",
        "    })\n",
        "    for (test, epsilon, alpha), row in summary.iterrows():\n",
        "        power_val = row['power']\n",
        "        se_val = row['se']\n",
        "        print(f\"  {test:20s} (ε={epsilon:.1f}, α={alpha:.3f}): Power = {power_val:.4f} ± {se_val:.4f}\")\n",
        "    \n",
        "else:\n",
        "    print(\"No power data available for plotting.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d7c6fa7",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d08a5c77",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "jiewAKXMuwAt"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv_notebook",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
